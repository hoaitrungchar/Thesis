seed_everything: 42

# ---------------------------- TRAINER -------------------------------------------
trainer:
  default_root_dir: "checkpoint/DiffInpant"
  min_epochs: 1
  max_epochs: 100
  precision: 32-true
  accelerator: cuda
  num_nodes: 1
  enable_progress_bar: true
  sync_batchnorm: True
  enable_checkpointing: True
  fast_dev_run: false
  logger:
    class_path: pytorch_lightning.loggers.CSVLogger
    init_args:
      save_dir: "checkpoint/DiffInpant/logs"
      name: null
      version: null
      prefix: ""

  callbacks:
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: "step"

    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        dirpath: "checkpoint/DiffInpant/checkpoints"
        monitor: "val/loss" # name of the logged metric which determines when model is improving
        mode: "min" # "max" means higher metric value is better, can be also "min"
        save_top_k: 1 # save k best models (determined by above metric)
        save_last: True # additionally always save model from last epoch
        verbose: False
        filename: "epoch_{epoch:03d}"
        auto_insert_metric_name: False

    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: "val/loss" # name of the logged metric which determines when model is improving
        mode: "min" # "max" means higher metric value is better, can be also "min"
        patience: 10 # how many validation epochs of not improving until training stops
        min_delta: 0. # minimum change in the monitored metric needed to qualify as an improvement

    - class_path: pytorch_lightning.callbacks.RichModelSummary
      init_args:
        max_depth: -1

    - class_path: pytorch_lightning.callbacks.RichProgressBar

# ---------------------------- MODEL -------------------------------------------
model:
  pretrained_path: ""
  beta_1: 0.9
  beta_2: 0.99
  lr: 5e-4
  weight_decay: 1e-5
  learn_sigma: True
  sigma_small: False
  noise_schedule: "linear"
  use_kl: False
  predict_xstart: False
  rescale_timesteps: False
  rescale_learned_sigmas: False
  timestep_respacing: ""

  net_init_mask:
    in_channels: 3
    out_channels: 1
  net_init_prior:
    in_channels: 3
    out_channels: 1
  net:
    image_size: 256
    in_channels: 5
    model_channels: 32
    out_channels: 6
    num_res_blocks: [1, 2, 2, 2, 2, 3, 4]
    attention_resolutions: [32, 16, 8]
    dropout: 0
    channel_mult: [1, 2, 4, 8, 8, 16, 16]
    conv_resample: True
    dims: 2
    num_classes: 2 
    use_fp16: False
    num_heads: 1
    num_head_channels: 64
    use_scale_shift_norm: True
    resblock_updown: False
    use_new_attention_order: False





# ---------------------------- DATA -------------------------------------------
data:
  path: "/home/vndata/trung/infer_image"
  mask_path_test: "/home/vndata/testing_mask_dataset"
  name_dataset: "FFHQ"
  prior: 'canny'
  batch_size: 1
  num_workers: 16
  pin_memory: False